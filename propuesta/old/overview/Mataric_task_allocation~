
In Intl. J. of Robotics Research 23(9):939-954, September 2004
A formal analysis and taxonomy of task allocation in multi-robot systems
Brian P. Gerkey Maja J Matarić

1 Introduction
Over the past decade, a significant shift of focus has occurred in the field of mobile robotics as researchers have begun to investigate problems involving multiple, rather than single, robots. From early work on loosely-coupled  tasks such as homogeneous foraging (Matari´c 1992) to more recent work on team coordination for robot soccer (Stone & Veloso 1999), the complexity of the multi-robot systems being studied has increased. This complexity has two primary sources: larger team sizes and greater heterogeneity of robots and tasks.

1.1 Multi-Robot Task Allocation (MRTA)
As a result of the growing focus on multi-robot systems, multi-robot coordination has received significant attention. In particular, multi-robot task allocation (MRTA) has recently risen to prominence and become a key research topic in its own right.

3 Utility
To treat task allocation in an optimization context, one must decide what exactly is to be optimized. Ideally the goal is to directly optimize overall system performance, but that quantity is often difficult to measure during system execution.

Utility is a unifying, if sometimes implicit, concept in economics, game theory, and operations research, as well as in multi-robot coordination. It is based on the notion that each individual can internally estimate the value (or the cost) of executing an action. Depending on the context, utility is also called fitness, valuation, and cost. Within multi-robot research, the formulation of utility can vary from sophisticated planner-based methods (Botelho & Alami 1999) to simple sensor-based metrics (Gerkey & Matarić 2002b). We posit that utility estimation of this kind is carried out somewhere in every autonomous task allocation system, for the heart of any task allocation problem is comparison and selection among a set of available alternatives. Since each system uses a different method to calculate utility, we give the following generic and practical definition of utility for multi-robot systems.


It is assumed that each robot is capable of estimating its fitness for every task it can perform. This estimation includes two factors, which are both task- and robotdependent:
- expected quality of task execution, given the method and equipment to be used (e.g., the accuracy of the map that will be produced using a laser range-finder),
- expected resource cost, given the spatio-temporal requirements of the task (e.g., the power that will be required to drive the motors and laser range-finder in order to map the building).

Given a robot R and a task T, if R is capable of executing T, then one can define, on some standardized scale, QRT and CRT as the quality and cost, respectively, expected to result from the execution of T by R. This results in a combined, nonnegative utility measure: 

URT =  	QRT - CRT if R is capable of executing T and QRT > CRT
          		0 otherwise
          		

**********************************************************************************************************************************************************
Proceedings of the 2002 IEEE lntemational Conference on Robotics & Automation Washington, DC May 2002
Multi-robot Task Allocation in the Light of Uncertainty
Esben H. @stergaard2TM aja J Matarić, and Gaurav S. Sukhatmel


Abstract
We describe an empirical study that sought general guidelines for task allocation strategies in multi-robot systems. We identify four distinct task allocation strategies, and demonstrate them in two versions of the multi-robot emergency handling task. We describe an experimental setup to compare results obtained

1 Introduction
There has been significant prior research in multi-robot coordination [1, 12, 2, 14, 3, 15, 71. We view this problem as an instance of dynamic task allocation. Presently, a general theory of task allocation for multi-robot domains remains elusive. This paper empirically derives general guidelines for selecting task allocation strategies for multirobot systems. The guidelines are necessarily incomplete given the empirical nature of the work. We demonstrate that the choice of task allocation strategy is far from trivial. We also empirically show that no optimal task allocation strategy exists for all domains, and that it can be very difficult to identify the optimal task allocation strategy even for a particular task. 

These results are derived through the use of a framework developed for understanding the task allocation problem, which illustrates a common approach to decomposing the problem. Using this framework, we compare four distinct task allocation strategies, in both grid world and real world task allocation experiments, applied to the emergency handling problem domain.

2 Problem Statement
In the context of multi-robot coordination, dynamic task allocation can be viewed as the selection of appropriate actions [10] for each robot at each point in time so as to achieve the completion of the global task by the team as a whole. From a global perspective, in multi-robot coordination, action selection is based on the mapping from the combined robot state space to the combined robot action space. For homogeneous robots, it is the mapping S|R| + AIRI, where S is the state space of a robot, |R| is the number of robots, and A is the set of actions available to a robot [11]. In practice, even with a small number of robots, this is an extremely high-dimensional mapping,  a key motivation for decomposing and distributing control. In [6], a system is described that decomposed the task into the following three steps: 1) each robot bids on a task based on its perceived fitness to perform the task; 2) an auctioning mechanism decides which robot gets the task; 3) the winning robot’s controller performs a sequence of actions to execute the task. In [14], each robot’s ability to perform a task is mapped to a scalar quantity, which is used to assign tasks to robots. In [15], a local eligibility mechanism is described as the robots’ perceived ability to perform a task.

We use the approach from [6] to construct a general formulation for the multi-robot coordination problem. In this formulation, a bidding function determines each robot’s ability to perform a task based on that robot’s state. Next, the task allocation mechanism determines which robot should perform a particular task based on the bids. Finally, the robot controllers determine appropriate actions for each robot, based on the robot’s current task engagement.

it reduces the dimensionality of the coordination problem, and it reduces the amount of inter-robot communication required. Instead of a mapping S|R| + A|R| we now have the mapping B|R||T| + T|R| (all robots’ bids for all tasks to a task assignment for each robot).

3 Four Task Allocation Strategies
Here we consider a Markovian system, where the task allocation mapping for a given robot is from that robot’s current task assignemnts and every robot’s current bid on each task, to the given robot’s new task assignment, Given each robot’s bid on each task and each robot’s current task engagement, what should each robot’s new task assignment be? We explored the effects that commitment and coordination have on performance in the context of four task allocation strategies. These four were derived from the combination of two variables: the amount of commitment to a given task engagement, and the amount of coordination among the robots

Along the commitment axis we examined a fully committed strategy and a fully opportunistic strategy. Along the coordination axis we examined an (uncoordinated) individualistic strategy and a (highly-coordinated) mutually exclusion strategy, where no two robots were allowed to be engaged in the same task at the same time.

As an example, one of the four algorithms we tested, the fully committed mutually exclusive strategy, looks as follows:
1. If a robot is currently engaged in a task, and its bid on that task is greater than zero, remove the row and column of the bid from the table, and set the robot’s new assignment to its current one.
2. Find the highest bid in the remaining table. Assign the corresponding robot to the corresponding task. Remove the row and column of the bid from the table.
3. Repeat from step 2 until there are no more bids

5 Discussion
In general, it is desirable to know whether the tendencies derived from the grid world simulation apply to the real world in our problem domain. The results from the real world experiments imply that the noise levels correspond  to the regime of the lower left of the space shown in Figure 8, in that the combination of mutual exclusion and opportunism was the best performing strategy. Further experiments could determine whether the correspondence between the grid world and the real world results is a coincidence or a systematic trend. One alternative is to perform more trials with the existing setups. Another is to design a third setup where further noise is added to the system.

The fact that the best performing task allocation strategy changes as we vary noise parameters in the grid world implies that it can be very difficult to decide apriori which task allocation strategy should be used in a given task for any real world implementation. From the grid world results, it seems that the benefit from mutual exclusion is dependent on the total noise in the system, while the benefit of commitment seems to be dependent on the ratio between “actuator noise” and “sensor noise”. Part of this trend is also acknowledged and utilized in [9], for a controller that, when noise is increased, degrades gracefully from a mutually excluding to an individualistic strategy.

6 Conclusion
We have described an empirical study that sought general guidelines for task allocation strategies in systems of multiple cooperating robots. We identified four distinct task allocation strategies, and demonstrated them in two versions of the multi-robot emergency handling task. We described an experimental setup to compare results obtained from a simulated grid world to the results from real world experiments. Data resulting from eight hours of real mobile robot experiments are compared to the trend identified in simulation. The data from the simulations show that there is no single strategy that produces best performance in all cases, and that the best task allocation strategy changes as a function of the noise in the system. This result is significant, and shows the need for further investigation of task allocation strategies.

[1] Ronald C. Arkin. Cooperation without communication: Multiagent schema based robot navigation. Journal of Robotic Systems, 9(3):351-364, April 1992. 
[2] T. Balch and R. Arkin. Behavior-based formation control for multi-robot teams. IEEE Transactions on Robotics and Automation, 14(6):1-15, 1998.
[3] Y. Cao, A. Fukunaga, A. Kahng, and F. Meng. Cooperative mobile robotics: Antecedents and directions. Autonomous robots, 4(1):7-27, 1995. 
[6] Brian Gerkey and Maja J Matarit. Principled communication for dynamic multi-robot task allocation. In D. Rus and S. Singh, editors, Experimental Robotics VII, WCIS 271, pages 353-362. Springer-Verlag Berlin Heidelberg, 2001.
[10] P. Maes. Modeling adaptive autonomous agents. Artificial Life, I, (1&2)(9), 1994. 
[I 11 Maja J Matarić. Interaction and intelligent behavior. Technical Report AI-TR-1495, MIT Artificial Intelligence Lab, 1994.
[ 121 Maja J Matarić. Issues and approaches in the design of collective autonomous agents. Robotics and Autonomous System, 16(24):321-331, December 1995.
[13] Esben H. Ostergaard, Maja J. Matarić, and Gaurav S. Sukhatme. Distributed multi-robot task allocation for emergency handling. In Proc. IEE/RSJ International Conference on Robots and Systems, (IROS), pages 821-826, Maui, Hawaii, 200 1.
[14] L. Parker. Alliance: An architecture for fault-tolerant multirobot cooperation. IEEE Transactions on Robotics and Automation, 14(2):220-240, 1998.
[I51 B. Werger and M. Matarić. Broadcast of local eligibility for multi-target observation. In Proceedings, 5th International Symposium on Distributed Autonomous Robotic Systems (DARS), Knoxville, TN, Oct 4-6, pages 347-356, Oct. 2000.
**********************************************************************************************************************************************************
In RoboCup 2003: Robot Soccer World Cup VII, LNCS Vol. 3020
Daniel Polani, et al., eds, pages 43-53, Springer-Verlag, Berlin Heidelberg, 2004
On role allocation in RoboCup
Brian P. Gerkey1 and Maja J Matari?


Abstract. A common problem in RoboCup is role allocation: given a team of players and a set of roles, how should be roles be allocated to players? From this perspective, we analyze the allocation mechanisms of a number of RoboCup teams, showing that most of them are greedy, and that many are in fact equivalent, as instances of the canonical Greedy algorithm. We explain how optimal, yet tractable, assignment algorithms could be used instead, but leave as an open question the actual benefit in terms of team performance of using such algorithms.

2 The problem
When studying the problem of multi-robot role allocation we take inspiration from operations research, a field that concerns itself with human organizations. In particular we claim that role allocation can be reduced to an iterated form of the Optimal Assignment Problem (OAP) [4], a well-known problem from operations research. A recurring special case of particular interest in several fields of study, this problem can be formulated in many ways. Given our application domain, it is fitting to describe the problem in terms of jobs and workers. There are n workers, each looking for one job, and n available jobs, each requiring one worker. The jobs can be of different priorities, meaning that it is more important to fill some jobs than others. Each worker has a nonnegative skill rating estimating his/her performance for each potential job (if a worker is incapable of undertaking a job, then the worker is assigned a rating of zero for that job). The problem is to assign workers to jobs in order to maximize the overall expected performance, taking into account the priorities of the jobs and the skill ratings of the workers. This problem was first formally studied in the context of assigning naval personnel to jobs based on the results of aptitude tests [5].

Our multi-robot role allocation problem can be posed as an assignment problem in the following way: given n robots, n prioritized (i.e., weighted) single-robot  role, and estimates of how well each robot can be expected to play each role, assign robots to roles so as maximize overall expected performance. However, because the problem of role allocation is a dynamic decision problem that varies in time with phenomena including environmental changes, we cannot be content with this static assignment problem. Thus we complete our reduction by iteratively re-solving the static assignment problem over time.

Of course, the cost of running the assignment algorithm must be taken into account. At one extreme, a costless algorithm can be executed arbitrarily quickly, ensuring an eficient assignment  over time. At the other extreme, an expensive algorithm that can only be executed once will produce a static assignment that is only initially eficient and will degrade over time. Finally there is  the question of how many roles are considered for (re)assignment at each iteration. In order to create and maintain an eficient allocation, the assignment algorithm must consider (and potentially reassign) every role in the system. Such an inclusive approach can be computationally expensive and, indeed, some implemented approaches to role allocation use heuristics to determine a subset of roles that will be considered in a particular iteration.


2.1 Utility
Utility is a unifying, if sometimes implicit concept in economics, game theory, and operations research, as well as multi-robot coordination.


2.2 Formalism
We are now ready to state our role allocation problem as an instance of the OAP. Formally, we are given:
- the set of n robots, denoted I1; : : : ; In
- the set of n prioritized roles, denoted J1; : : : ; Jn and their relative weights w1; : : : ;wn
- Uij , the nonnegative utility of robot Ii for role Jj , 1 <= i, j <= n
We assume:
- Each robot Ii is capable of executing at most one role at any given time.
- Each role Jj requires exactly one robot to execute it.

The problem is to find an optimal allocation of robots to roles. An allocation is a set of robot-role pairs: (i1; j1) ... (in; jn)

If the robots' utilities can be collected at one machine (or distributed to all machines), then a centralized allocation mechanism can be employed. This is often the case in RoboCup,where locally-computed utility values are generally either unicast to a single player/coach or broadcast to all players. In the former case, one machine can execute the allocation algorithm and inform the players of the results; in the latter, all players can execute the allocation algorithm in parallel4. 

Perhaps the most common role allocation technique is the following 
1. Assemble the utility values into an n x n matrix.
2. Find the highest utility uij , assign robot i to role j, and cross out row i and column j from the utility matrix.
3. Repeat step 2 until all roles have been assigned.

In place of this greedy approach, it is possible to employ optimal solutions, a great many of which can be found in the literature [14].  The best-known approach is the Hungarian method [15], a linear programming algorithm that exploits the special structure of the OAP.	This algorithm will find the optimal solution to a role allocation problem in O(n3) time. We have demonstrated empirically [16] that the constant factor for the Hungarian method is so small that this algorithm could easily be used for real time role allocation in RoboCup teams, where n <= 11.

It is also possible to solve assignment problems in a completely distributed fashion. Viewing the OAP as a "stable marriage" problem [17]: given a group of n boys and a group of n girls, each with preferences (i.e., utilities) over the members of the opposite sex, find a set of pairings such that there exists no boy and girl pair that is not paired together but prefers each other to their current mates. Gale and Shapley developed an intuitive algorithm in which each boy proposes to his favorite available girl and each girl conditionally accepts her favorite proposer. This process is repeated in a series of stages until, when the last girl has received a proposal, the "courtship" is declared to be over and the result is an optimal solution to the problem of assigning boys to girls so as to maximize total utility.

4. Gale, D.: The Theory of Linear Economic Models. McGraw-Hill Book Company, Inc., New York (1960)
5. Thorndike, R.L.: The Problem of Classi¯cation of Personnel. Psychometrika 15 (1950) 215{235
14. Bertsekas, D.P.: The Auction Algorithm for Assignment and Other Network Flow Problems: A Tutorial. Interfaces 20 (1990) 133{149
15. Kuhn, H.W.: The Hungarian Method for the Assignment Problem. Naval Research Logistics Quarterly 2 (1955) 83{97
16. Gerkey, B.P., Matarić, M.J.: A formal framework for the study of task allocation in multi-robot systems. Technical Report CRES-03-013, Center for Robotics and Embedded Systems, School of Engineering, University of Southern California (2003)
17. Gale, D., Shapley, L.S.: College Admissions and the Stability of Marriage. American Mathematical Monthly 69 (1962) 9{15
************************************************************************************************************************************
A machine learning method for improving task allocation in distributed multi-robot transportation
Torbjorn S. Dahl Maja J Matarić and Gaurav S. Sukhatme

Machine learning [24] is a means of automatically generating solutions that perform better than those that are hand-coded by human programmers; Such improvement is possible in problem domains where optimal solutions are difficult to identify, i.e., when there are no models available that can accurately relate a system's dynamics to its performance. . We present a gen- eral behavior-based algorithm that uses reinforcement learning to improve the spatio-temporal organization of a homogeneous group of robots. In this algorithm each robot applies the learning at the level of individual behavior selection. We demonstrate how the interactions within the group affect the individual learning in a way that produces group-level effects, such as lane-formation and specialization, and improves the group's performance. We also present a model of multi-robot task allocation as resource distri- bution through vacancy chains, a distribution method common in human and animal societies, and an algorithm for multi-robot task allocation based on that model. The model explains and predicts the task allocation achieved by our algorithm and highlights its limitations.

The behavior-based (BB) control paradigm [22, 1] provides a means of structuring robot controllers into collections of task-achieving modules or behaviors, such as exploration and obstacle avoidance. The modules operate in parallel and interact within the system and also through their effects on the environment. When properly designed, the resulting controller produces robust, repeatable, and reliable overall behavior for the robot. ML can be employed at the level of behavior coordination, or at the level of the behaviors themselves. The behaviors typically have a set of preconditions used to decide when they are suitable for execution.

1.2.1 Group dynamics

By the term group dynamics, we refer to the interaction among the members of a group of robots. Group dynamics can have positive or negative effects on the group's performance. Interference is a typical negative effect in confined spaces, but positive or synergistic effects are also possible when the actions of one robot unintentionally facilitates the actions of another robot, or when robots intentionally cooperate. Each member of a group of robots is a physically embedded system with inherent uncertainty. Interactions among a group of physically embodied systems increase uncertainty to a level where practical accurate models are extremely diffcult to construct. Previous work on modeling group dynamics can be divided into microscopic (simulation-based or otherwise) and macroscopic (describe system properties) [19].

1.3 Cooperative transportation
In transportation problems, a group of robots traverses a given environment in order to transport items between sources and sinks. We call the time taken to traverse the environ- ment once from a sink via a source and back to a sink again the traversal time. We call the time between two arrival events at either source or sink the target time. To perform optimally the robots must maximize the number of traversals over time.

The basic transportation problem is one of the sub-problems of foraging [2, 17]. Foraging is reduced to a problem of transportation when the locations of the sources and sinks are known. We refer to a problem as a prioritized transportation problem when sources and sinks have (sets of) different values.

1.4 Improving performance on cooperative tasks

Taking advantage of the reduction in state/action space made possible by the BB approach, we implemented a general algorithm, based on distributed re- inforcement learning [28], that adapts the spatio-temporal properties of each robot's behavior in a way that improves the group's performance on cooperative tasks [9]. The robots learn from performance-related feedback. In homogeneous groups this feedback mainly reflects effects of the prevailing group dynamics and allows the robots to adjust their behavior so as to minimize the negative effects and maximize the positive ones.

1.4.1 The learning algorithm

The algorithm for spatio-temporal organization is built on two main principles: Individual learning and emergence. Each robot individually estimates the utility of the available actions and chooses from these in a way that allows it both to keep its estimates current through continuous experimentation and to exploit the actions with high estimated utilities. We used Q-learning for utility  estimation. 

Our algorithm depends on a reward structure that reflects the effects of the current group dynamics. In order to use Q-learning to optimize performance over time it is necessary to make the temporal aspect of performance explicit in the reward function r = wi/t (where wi = task value and t = last task processing time). Using a reward function that is sensitive to the effects of the group dynamics allows the robots to learn to take actions that minimize these effect with respect to their individual performance.

1.4.2 Experimental validation of spatio-temporal organization

We hypothesized that it was be possible for recognizable spatio-temporal features of group behavior such as turn-taking, lanes, and territories to emerge in a group of robots using our adaptive algorithm. To test this hypothesis we choose to the problem of cooperative transportation, a problem that could be given clear spatial and temporal constraints.  The robots wore color markings recognizable using ActiveMedia's Color-Tracking Software (ACTS), allowing them to perceive each other's orientation.

Controller architecture To support  flexibility in the spatio-temporal features of the transportation, we implemented three transport-related behaviors: 
- direct target approach: servoed the robot directly toward the current target.
- wall following: followed the boundaries of the simulated environment, eventually arriving at a source or a sink.
- stopping: froze the robot in its current position and orientation.

 These three behaviors made use of lower level behaviors such as obstacle avoidance. The mapping of the transport-related behaviors to input states was done by the learning algorithm presented in Section 1.4.1.
 
 Learning parameters The input state presented to the learning algorithm consisted of five bits. The first bit indicated the presence of a source or a sink. The second bit was on for random intervals of time (1-30 seconds), allowing the robot to express certain behaviors for short periods independent of the rest of the input state. For example, instead of always following the walls  whenever a target is not visible, the robot could learn to approach the target directly most of the time in this state, interspersed with short periods of wall following. The three final bits represented the presence of color-blobs corresponding to the three colored markings on the robots, indicating the presence and orientation of another robot. When several robots were present, these bits reported only on the visible markings of the closest one, distinguished by the height of the visible color-blobs.
 
 Analysis By analyzing the Q-tables produced during this experiment we found that the robots learned policies dominated by the direct target approach behavior, but with significant portions of the wall-following behavior. These policies made the robots follow a common circular path from the source to the sink and back, while keeping to the left side to minimize the interference with robots coming from the opposite direction. The policies show that a structuring of the group's spatial organization is partly responsible for the improvement in performance.
 
 We also found evidence that the robots adapted their temporal organization according to the presence and orientation of other robots. This indicates that the robots in general learned to stop only when they saw a robot without visible yellow markings. Intuitively, visible yellow or rear markings indicate that the other robot is facing, and likely moving, away, and hence not likely to cause interference.

1.5 Task-allocation through specialization
MRTA is a problem domain that presents a third major challenge, in addition to those we reviewed in Sections 1.1 and 1.4. This challenge concerns the fact that if the effects of the group dynamics are considered explicitly, MRTA is an NP-complete problem [10]. This inherent complexity of MRTA implies that it is not feasible to find optimal solutions as the size of the system, i.e., the number of tasks and robots, grows. For such high-complexity problems, solutions must be found through heuristic algorithms that rely on appropriate assumptions about the problem to construct solutions of the necessary quality.

In this section we present the TAVC model of MRTA and the associated TAVC algorithm [11]. Our TAVC model explicitly represents the effects of group dynamics and allowed us to develop the TAVC algorithm, whose main contribution is its sensitivity to those effects. The TAVC algorithm uses Q-learning to make individual estimates of task utilities; in it each robot follows a greedy rule for choosing its next task based on its estimated utility. The algorithm goes beyond traditional greedy algorithms [8] in that the utility estimates are sensitive  to the effects of the group dynamics produced by the current task allocation.


 1.5.1 Multi-robot task allocation 

Multi-robot task allocation (MRTA), is the problem of allocating a set of tasks to the robots in a group in a way that optimizes some group-level performance metric. This problem is receiving  growing attention. Recently, Gerkey and Matarić [14, 13] presented a task- and architecture-independent framework in which to study and compare existing empirical solutions. The solution methods include L-ALLIANCE [26], a system for MRTA based on local task allocation decisions only. Robots using L-ALLIANCE individually estimate their own progress and the progress made by other robots, and take over or acquiesce tasks according to those estimates. However, the progress estimation functions are problem-specific and there is no explicit consideration of the effects of interaction. 
 
 The M+ algorithm [4] uses a centralized task allocation mechanism based on negotiation through the Contract Net Protocol. The algorithm relies on pre-specified capabilities and does not consider effects of interaction. Also, a distributed negotiation system carries with it a high communication overhead that introduces problems when scaling the system up to handle larger numbers of robots.
 
Similarly, MURDOCH [12] used a negotiation mechanism based on the Contract Net Protocol for task allocation. However, MURDOCH leaves open a suitability parameter to be filled in by the individual robots when bidding for tasks. Though it is was done in their work, the suitability value could reject the effects of the group dynamics. The broadcast of local eligibility algorithm [29] defines a similar parameter called eligibility. The eligibility value is calculated by the robots individually and used in an inter-robot coordination mechanism based on port-arbitrated behaviors (PABs). PABs are a generalization, for multi-robot systems, of the traditional behavior coordination mechanisms. As in MURDOCH, the locally calculated eligibility could reflect the effects of the group dynamics but was not used in that manner. 

Among previous MRTA algorithms, none deal explicitly with the effects of group dynamics. Our TAVC algorithm addresses that deffciency.  

1.5.2 The vacancy chain model

The typical example of resource distribution through a vacancy chain is a bureaucracy, where the retirement of a senior employee creates a vacancy to be filled by a less senior employee. The promotion, in turn, creates a second vacancy to be filled, and so on. Inspired by this process we developed a model of MRTA that describes how task values and task processing times combine to produce task allocations among greedy consumers [10]. Our model has the important property of breaking the group performance down into individual robot contributions, a property that allows us to use distributed control algorithms.

According to our TAVC model, any number of robots can be assigned to tasks from a given class. When a robot, j, is assigned to a task from a class, i, currently being serviced by a set of robots, J, we say that service-slot, (i, J, j) is being filled. A particular set of robots, J, servicing the same task, i, will have a task processing frequency, ci,J , dependent on the degree to which the robots are able to work concurrently without interference. The difference in the group's task processing frequency before and after robot j joined, together with the task value, wi, defines robot j's contribution. We call this contribution, which can be negative, the slot-value, si,J,j . The formal definition of the slot-value is given by: 
si,J,j = wi(ci, J U {j} - ci,J)

Assigning a robot to a task can lead to a decrease in the task processing frequency of the group of robots servicing tasks from that class. The slot-value then becomes negative. When all the available service-slots for a class of tasks have negative slot-values, we say the task is saturated. 


1.5.3 The vacancy chain algorithm
In the TAVC algorithm, a task corresponds to an action in RL terms. Each robot uses a Q-table to keep individual estimates of task utilities and choses its next task using an e-greedy selection function.

 The task processing is implemented in terms of pre-programmed high-level behaviors, one per each of the available tasks. The state-space describes what class of tasks the robot last serviced. This state-space allows the robots to learn policies that are not dedicated to one class of tasks; the learned policies can switch between classes of tasks in order to construct optimal task sequences. The action space corresponds to the available classes of tasks.
 
 1.5.4 Experimental validation of TAVC

 The simulated environment The experiments presented in this section were done in simulation using six robots identical to those described in Section 1.4.2. The simulated environment was an 8 by 12-meter rectangular area with two unique laser bar-codes along one of the longer sides of the rectangle, indicating sources, and two along the opposing side indicating sinks.
 
 Controller architecture All of the task-related behaviors used by the TAVC algorithm were implemented as a collection of sub-behaviors responsible for different aspects of a safe and eficient task execution. These sub-behaviors were: 
 
- obstacle avoidance: avoided obstacles detected (by laser range finder) in the desired path.
- visible target approach: approached the desired target when it was visible (to the laser range finder).
- target location approach: approached the location of the desired target when the target itself was not visible.
- wall following: followed the walls to the first available target when the desired target was not visible and localization (based on odometry) was deemed to be inaccurate.

Learning parameters The Q-tables ware initialized with random values be- tween -0:1 and 0:1, the learning rate, (alpha), was set to 0:1, and the discount factor (gamma)  was set to 0:95. For action selection we used a greedy-e strategy [28], where e was set to 0:1

In order to produce this task allocation, a situation needed to be set up in which it was less attractive to be one of four robots servicing the high-value circuit than to be one of three servicing the low-value circuit. This constraint on the reward function is presented formally by:
V (x, y).rx,4 < ry,3

We also wanted to demonstrate the filling of a vacancy on the high-value circuit. In order for a vacancy in the high-value circuit to be filled, it had to be the case that it was more attractive to be the third robot on that circuit than to be the third robot on the low-value circuit.
V (x != p).rx,3 < rp,3

1.5.6 Conclusions
In the `initial task distribution' experiment we showed that the performance of the TAVC algorithm was significantly above the performance level of a system where tasks were allocated at random. In the `vacancy from robot breakdown' experiment we showed that the performance of the TAVC algorithm after re-convergence was higher than a misconfigured pre-programmed solution. Assuming that a model of the group dynamics is not available in general, a pre-programmed solution must be based on some heuristics that will occasionally lead to sub-optimal task  allocations. Our experiment shows that the TAVC algorithm has the potential to improve on pre-programmed solutions.
**********************************************************************************************************************************************************
IEEE TRANSACTIONS ON ROBOTICS AND AUTOMATION, VOL. 18, NO. 5, OCTOBER 2002

Sold!: Auction Methods for Multirobot Coordination
Brian P. Gerkey and Maja J. Matarić

HOW CAN WE intelligently coordinate groups of robots? The key to exploiting the potential of multirobot systems is cooperation. Robots should, whenever possible, cooperate strongly in order to maximize their overall task performance. By sharing information and leveraging each others’ skills, a group of robots can truly be more than the sum of its parts.

How can we achieve such cooperation in systems composed of failure-prone autonomous robots operating in noisy, dynamic environments? One model, especially popular in the field of swarm robotics [1], [2], is emergent cooperation. However, it is a solution to a specific problem, and if robots are to be generally useful, they must be capable of solving a variety of problems. Furthermore, emergent cooperation solutions have traditionally been applied to homogeneous robot groups and have relied heavily on redundant skills across the group to achieve good overall performance.

For our general strategy, we choose instead an intentional model of cooperation [3]. In this model, robots cooperate explicitly and with purpose, often through task-related communication. As compared with emergent cooperation, intentional cooperation is better suited to the kinds of real-world tasks that we, as humans, might want robots to do. In this paper, our use of intentional cooperation is at the level of task allocation and need not propagate to the level of task execution.

In deciding who should do what in a team, we take inspiration from the distributed artificial intelligence (DAI) community. Specifically, we employ fitness-based auctions and a simple negotiation protocol. The concept of negotiation necessarily entails some form of task commitment, which can complicate system design and might hinder performance or fault tolerance [4]. However, a large class of tasks, especially those involving physical state, can benefit from the robots’ willingness to commit. Furthermore, as we demonstrate, task allocation based on explicit negotiation can be an effective and faulttolerant method for controlling multirobot systems. Thus, this paper answers the challenge posed in [3]:
“[Negotiation-based] solutions have not been adequately demonstrated in situated agent (i.e., robotic) teams, which have to live in and react to dynamic and uncertain  environments amidst noisy sensors and effectors, frequent agent failures and a limited bandwidth, noisy communication mechanism.”


II. THE PROBLEM
As stated above, we are concerned with the problem of multirobot coordination. In this paper, we attack one part of that problem: task allocation for groups of robots. We characterize our domain with the following assumptions:
• the system is composed of physically embodied robots;
• the robots are heterogeneous, possessing different skills;
• the robots can communicate, but messages may be lost;
• the robots are honest and cooperative;
• the robots (or parts of them) can fail at any time;
• a robot may not be aware of its own (partial) failure;
• the robots are multipurpose, not task specific;
• no model is available to describe the sequence in which tasks are generated;
• if a robot is to oversee a task, it can determine its own progress and completion of the task.

In order to reason clearly about multirobot task allocation, we cast it as a dynamic resource-allocation problem. Given a set of resources and a sequence of tasks, the goal is to assign resources
to tasks in an efficient manner. In its most general form, this problem is equivalent to the NP-complete conjunctive planning problem [6].

Clearly, the traditional planning approach will not suffice for the physical robot domain with which we are currently concerned. Aside from the well-known problems of providing timely responses and reacting to unexpected contingencies, a planning system is especially unsuited to our task-allocation problem because, from the perspective of the robots, which are model free, the tasks appear to be randomly generated






**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************
**********************************************************************************************************************************************************

